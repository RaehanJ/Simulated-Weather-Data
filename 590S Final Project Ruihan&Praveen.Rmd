---
title: "Final Project: Simulation Functional Data Analysis"
subtitle: "Functional Data Analysis With Application to United States Weather Data by Katherine S. King in 2014"
date: "MATH 590S, Spring 2023, Due Wednesday, May 3"
author: "Ruihan Jiang and Praveen Niranda Kumarasinghe Hetti Arachchige"
geometry: "left=1.5cm,right=1.5cm,top=1cm,bottom=1.5cm"
output: 
  pdf_document:
    keep_tex: yes
---

# Data Information
Similarities and differences in the use of data with the authors.

>- Similarities:

Same Source:
\newline National Oceanic and Atmospheric Administration (NOAA)'s National Climatic Data Center, from: https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/city/time-series/USW00024153/tavg/ann/12/1971-2023?base_prd=true&begbaseyear=1950&endbaseyear=2023

Same 16 Chosen Cities:
\newline Burlington, LA, Portland, Miami, Salt Lake City, Nashville, NYC, San Antonio, Indianapolis, Minneapolis, Atlanta, Green Bay, Missoula, Fairbanks, Boston, San Francisco

>- Differences:

Different Value:
\newline Monthly average temperatures from 1971-01 to 2023-01 were used here, whereas daily temperature from 1950 to 2013 was applied in the paper.

5 Missing Data:
\newline Missing data showed as '$-99$' in the raw data set, whereas paper used data without missing values, so we manually computed missing monthly average data from the website: https://www.extremeweatherwatch.com, then updated it.

Different Number of Time Points:
\newline 625 (= 12 months * 52 years + 1 month) were applied here, while there were $23,376$ data points used in the paper.

Different Period:
\newline 1 year (12 months) here, 365.25 days in the paper.

```{r loading, include=FALSE}
### Package needed
library(fda)

### Get Raw Data
t <- as.matrix(read.csv("monthlytemp.csv"))

### Modify Raw Data
t <- t[-c((nrow(t)-1):nrow(t)),]
y <- substr(as.character(t[,1]),1,4)
m <- substr(as.character(t[,1]),5,6)
rownames(t) <- as.character(paste0(y,'-',m))
temp <- list(date = as.character(paste0(y,'-',m)), data = t[,-1], Burlington = as.matrix(t[,2], ncol = 1), east = as.matrix(t[,c('Burlington', 'Miami', 'New.York.City', 'Boston')]), west = as.matrix(t[,c('Los.Angeles', 'Portland', 'Fairbanks', 'San.Francisco')]))
colnames(temp$Burlington) <- "Burlington"

## Check missing value
summary(temp$data)  # -99 means missing

### Get missing data from 
# 2004-12 23.69
temp$data[,'Burlington'][temp$data[,'Burlington'] == -99]
temp$data['2004-12','Burlington'] <- 23.69
# 1972-11 39.5
temp$data[,'Salt.Lake.City'][temp$data[,'Salt.Lake.City'] == -99]
temp$data['1972-11','Salt.Lake.City'] <- 39.5
# 1993-12 40.23
temp$data[,'Nashville'][temp$data[,'Nashville'] == -99]
temp$data['1993-12','Nashville'] <- 40.23
# 2001-04 42.45
temp$data[,'Missoula'][temp$data[,'Missoula'] == -99]
temp$data['2001-04','Missoula'] <- 42.45
# 2018-07 65
temp$data[,'San.Francisco'][temp$data[,'San.Francisco'] == -99]
temp$data['2018-07','San.Francisco'] <- 65

### Make Sure no Value Missing
summary(temp$data)
```

>- Create Raw Data List

Referring to the data set growth from the lecture notes (file: fda.lec1.R), we store our data in a list named 'temp' with three sublists. Sublist one named 'date' is the year-month vector; sublist two is the temperature data matrix with 625 rows (each represents one time point) and 16 columns (each represents one city); and list three is the one-column Burlington city temperature data matrix.

```{r view-data}
### View parts of the raw data
temp$data[1:6,1:6]
```

\newpage
# Transformation to Functional Data
The following steps were used and obtain figures matching those in the paper with overlapping time points, so it was reasonable to infer that the non-overlapping parts also made sense, and thus, the raw data was correctly transform to the functional data without the effects of seasonal variation.

```{r general-fixed-values, include=FALSE}
# Fixed Values
D <- seq(as.Date("1971-01-01"), as.Date("2023-01-01"), by = "month")
times <- as.numeric(y) + (as.numeric(m) - 1)/12
rangeval <- c(min(times), max(times))
```

>- Step 1: Choose number of basis function ($K=6$) to fit fourier series.

Although $min\{SSR\}$ was obtained when $K=46$, there was no big difference between SSRs at $K=46$ and $K=6$ (see Figure 1 below) which means when adding more basis functions the sum doesn't substantially decrease, so it was not worth the cost of complicating the model, that was why $K=6$ was chosen. Plots of seasonal fitting with fourier basis were periodic and didn’t vary from year to year in its period.

```{r K-fourier, echo=FALSE, fig.height=3, fig.width=4}
### Fixed Value
period <- 1
harmaccelLfd <- vec2Lfd(c(0, 2 * pi, 0), c(0, 1))


### Optimal # of nbasis
f_nbasiss = c(2:10)
f_SSEs = rep(0,length(f_nbasiss))
for(i in 1:length(f_nbasiss)){
  nbasis = f_nbasiss[i]
  fbasis = create.fourier.basis(rangeval, nbasis = nbasis, period = period)
	fdParobj = fdPar(fbasis, Lfdobj=harmaccelLfd)
	f_SSEs[i] = smooth.basis(times, temp$data, fdParobj)$SSE
}
plot(f_nbasiss, f_SSEs, type='o', xlab='K', ylab='SSR', sub = "Figure 1")
title(main = "Fourier Basis", adj = 0, line = 1.1)
abline(v = c(6, f_nbasiss[which.min(f_SSEs)]), col = "red", lty = 2)
```

```{r fourier, echo=FALSE,, fig.height=3.3, fig.width=15}
### Fit the fourier series
f_nbasis = 6
fbasis = create.fourier.basis(rangeval, nbasis = f_nbasis, period = 1)
f_fdParobj = fdPar(fbasis,Lfdobj=harmaccelLfd)
tempfd_fourier = smooth.basis(times,temp$data,f_fdParobj)

### Plots
plot(tempfd_fourier, xlab = "Year", ylab = "Temperature", sub = "Figure 2")
title(main = "All Cities Temperature Seasonal Fit", adj = 0, line = 1.1)
mtext("Fourier basis on raw data with K=6", side = 3, adj = 0)

plot(tempfd_fourier, xlim = c(1971, 1981), xlab = "Year", ylab = "Temperature", sub = "Figure 3")
title(main = "First Decade All Cities Temperature Seasonal Fit", adj = 0, line = 1.1)
mtext("Fourier basis on raw data with K=6", side = 3, adj = 0)

plot(smooth.basis(times,temp$Burlington,f_fdParobj), xlim = c(1971, 1981), xlab = "Year", ylab = "Temperature", sub = "Figure 4")
title(main = "First Decade Burlington Temperature Seasonal Fit", adj = 0, line = 1.1)
mtext("Fourier basis on raw data with K=6", side = 3, adj = 0)
```

\newpage
>- Step 2: Choose $K=109$ basis functions to fit the order 6 B-spline basis to the raw data.

Order 6 was chosen in order to estimate the first and the second derivatives well (smooth), the spline must have order four more than the derivative intended to be examined. Although $min\{SSR\}$ was obtained when $K=150$, there was no big difference between SSRs at $K=150$ and $K=109$ (see Figure 5 below). To keep the model as simple as possible, we chose $K=109$. This result was consistent with the result from the author since she also had the plot with the wired drop as what we got in Figure 5. Plots of seasonal fitting with B-spline basis were periodic too, but curves of B-spline basis plots slightly varied from year to year.

```{r K-B-spline, echo=FALSE, fig.height=3, fig.width=4}
### Fixed Value
norder = 6


### Optimal # of nbasis
bs_nbasiss = c(70:150)
bs_SSEs = rep(0,length(bs_nbasiss))
for(i in 1:length(bs_nbasiss)){
  nbasis = bs_nbasiss[i]
  bsplbasis = create.bspline.basis(rangeval, nbasis = nbasis, norder = norder)
	fdParobj = fdPar(bsplbasis, Lfdobj=int2Lfd(2))
	tempfd = smooth.basis(times, temp$data, fdParobj)
	bs_SSEs[i] = smooth.basis(times, temp$data, fdParobj)$SSE
}
plot(bs_nbasiss, bs_SSEs, type='o', xlab='K',ylab='SSR', main = "B-spline Basis", sub = "Figure 5")
abline(v = c(109, bs_nbasiss[which.min(bs_SSEs)]), col = "red", lty = 2)
```

```{r B-spline, echo=FALSE, fig.height=3.3, fig.width=15}
### Use optimal basis fit the B-Spline Basis
bs_nbasis = 109
bsbasis = create.bspline.basis(rangeval, nbasis = bs_nbasis, norder = norder)
bs_fdParobj = fdPar(bsbasis,Lfdobj=int2Lfd(2))
tempfd_bspline = smooth.basis(times,temp$data,bs_fdParobj)


### Plots
plot(tempfd_bspline, xlab = "Year", ylab = "Temperature", sub = "Figure 6")
title(main = "All Cities Temperature Seasonal Fit", adj = 0, line = 1.1)
mtext("Order 6 B-spline basis on raw data with K=109", side = 3, adj = 0)

plot(tempfd_bspline, xlim = c(1971, 1981), xlab = "Year", ylab = "Temperature", sub = "Figure 7")
title(main = "First Decade All Cities Temperature Seasonal Fit", adj = 0, line = 1.1)
mtext("Order 6 B-spline basis on raw data with K=109", side = 3, adj = 0)

plot(smooth.basis(times,temp$Burlington,bs_fdParobj), xlim = c(1971, 1981), xlab = "Year", ylab = "Temperature", sub = "Figure 8")
title(main = "First Decade Burlington Temperature Seasonal Fit", adj = 0, line = 1.1)
mtext("Order 6 B-spline basis on raw data with K=109", side = 3, adj = 0)
```

\newpage
>- Step 3: Subtract off the season component (Fourier series) from the previous spline using order 6 and $K=10$ basis functions instead of $K=109$ before penalty to “de-noise”.

$K=10$ was chosen in order to retain the shape of the curve, yet ensure the function did not exhibit excessive variation. The image now clearly shows us a trend of increasing temperature.

```{r deseasoned-data-before-penalty, echo=FALSE, fig.height=5, fig.width=16}
### Subtract off the season component from the previous spline
tempfd_deseasoned = eval.fd(times, tempfd_bspline$fd)-eval.fd(times, tempfd_fourier$fd)


### Choose a new nbasis=10
new_bsbasis = create.bspline.basis(rangeval, nbasis = 10, norder = norder)

### Get deseasoned functional data
deseasoned = Data2fd(times, tempfd_deseasoned, new_bsbasis)


### Plots
par(mfrow=c(1,2))
plot(deseasoned, ylim = c(-4, 4), xlab = "Year", ylab = "Temperature", sub = "Figure 9")
title(main = "All Cities Temperature Splines", adj = 0, line = 1.1)
mtext("Order 6 B-spline curve on deseasoned data with K=10", side = 3, adj = 0)

# Now smooth with a fourth-derivative penalty and a very small smoothing parameter
d0 = smooth.basis(times, tempfd_deseasoned, fdParobj = fdPar(new_bsbasis, Lfdobj = 4, lambda = 3269017))

plot(d0, ylim = c(-4, 4), xlab = "Year", ylab = "Temperature", sub = "Figure 11")
title(main = "All Cities Temperature Penalized Splines", adj = 0, line = 1.1)
mtext("Order 6 B-spline curve on deseasoned data with K=10", side = 3, adj = 0)
```

>- Step 4: Use generalized cross validation criteria (GCV) to penalty the de-seasoned functional data.

The smooth parameter $\lambda \approx 10^6$ was chosen based on the lowest GCV score so that the residuals from the curves without seasonal variation was minimized as much as possible but still allowed for a smooth curve. Splines of all the cities with seasonal variation removed penalized against the 4th derivative using $\lambda$ and $K=10$ basis functions. The main smoothing occurred by decreasing the number of knots used so Figure 9 and Figure 10 were a little bit different, Splines after penalty (Figure 9) was more smooth than those in Figure 8.  We can observe that there is an overall increasing trend in all curves.

```{r GCV, echo=FALSE, fig.height=3, fig.width=4}
lambdas = exp(seq(-1,26,by=4))
gcvs = matrix(rep(rep(0,length(lambdas)),16), ncol = 16, nrow = length(lambdas))
colnames(gcvs) <- colnames(temp$data)
rownames(gcvs) <- lambdas
for(i in 1:length(lambdas)){
	fdParobj = fdPar(new_bsbasis, Lfdobj=int2Lfd(2), lambda=lambdas[i])
	precfd = smooth.basis(times,tempfd_deseasoned,fdParobj)
	gcvs[i,] = precfd$gcv
}

plot(1:7, gcvs[,1], type = "o", xlab = "ith smooth parameter", ylab = "GCV score", sub = "Figure 10", main = "GCV Criteria")
abline(v = 5, col = "red", lty = 2)

# d0 = smooth.basis(times, tempfd_deseasoned, fdParobj = fdPar(new_bsbasis, Lfdobj = 4, lambda = lambda[5])) <- this is removed forward to make pdf looks better
```

\newpage

# 2 Units Standard Deviation Bounds (95% Confidence Limit) for Mean and Mean Derivative
Next we can look at the average of all cities together.

The standard deviation of the mean was doubled and added to both sides of the mean function to represent the mean with standard deviation limits above and below, shown in Figure 12 and Figure 13. Using a more precise calculation, the standard error covariance matrix was calculated and used to create a 95% confidence limit for the different cities in the study.

There is indeed an upward trend in the average temperature of all cities(Figure 12). The highest bound for average temperature in 1970 is significantly smaller than the lower bound for that in 2023. So the average temperature has gone up by around 2 degrees. We can also see that the lower limit is above the zero mark after late 1990s and the upper limit is below the zero mark before early 1990s. 

Furthermore the mean function is not a straight line so this pattern of upward
momentum is not fully linear. 

When observing the rate of change of the average temperature (Figure 13) we can see that it undergoes some dynamics around 1980 and 2000. Furthermore there the rate of change, although it's small, it's always positive after the late 1970s. This further confirms the rise of average temperature over time. 
Moreover, after late 1970s even the lower bound for rate of change is positive meaning temperatures were in fact increasing at a significant rate.

We may look at temperature of individual cities for similar patterns (Figure 14 and 15).

```{r 95ci, echo=FALSE, fig.height=6, fig.width=8}
par(mfrow=c(2,2))
# All cities
ncurves = 16
mean0 <- mean.fd(d0$fd)
std0 <- std.fd(d0$fd)
std0$coefs <- std0$coefs/sqrt(ncurves)
plot(d0, ylim=c(-2,2), col = "lightgrey", xlab = "Year", ylab = "Temperature", sub = "Figure 12")
lines(mean0)
lines(mean0+qnorm(0.975)*std0, lty = 6, col = "red")
lines(mean0-qnorm(0.975)*std0, lty = 6, col = "blue")
title(main = "95% C.L. for the Mean", adj = 0, line = 1.1)
legend("topright", legend = c("Mean", "Mean + 2*SD", "Mean - 2*SD"), col = c("black", "red","blue"), lty = c(1, 6, 6), bty = "n")
mtext("Penalized splines for deseasoned data", side = 3, adj = 0)

d1 <- deriv.fd(d0$fd)
mean1 <- mean.fd(d1)
std1 <- std.fd(d1)
std1$coefs <- std1$coefs/sqrt(ncurves)
plot(d1, ylim = c(-0.2,0.2), col = "lightgrey", xlab = "Year", ylab = "Temperature", sub = "Figure 13")
lines(mean1)
lines(mean1+qnorm(0.975)*std1, lty = 6, col = "red")
lines(mean1-qnorm(0.975)*std1, lty = 6, col = "blue")
title(main = "95% C.L. for the Mean of Derivative", adj = 0, line = 1.1)
legend("topright", legend = c("Mean", "Mean + 2*SD", "Mean - 2*SD"), col = c("black", "red","blue"), lty = c(1, 6, 6), bty = "n")
mtext("Penalized splines for deseasoned data", side = 3, adj = 0)

# Burlington
bur_fd <- smooth.basis(times,tempfd_deseasoned[,"Burlington"], fdParobj = fdPar(new_bsbasis, Lfdobj = 4, lambda = lambdas[5]))$fd

bur_fd1 <- deriv.fd(bur_fd)

plot(bur_fd, ylim=c(-2,2),  xlab="Year", ylab="Temperature", lty = 1, sub = "Figure 14")
title(main = "95% C.L. for Burlington", adj = 0, line = 1.1)
lines(bur_fd+qnorm(0.975)*std0, lty = 6, col = "red")
lines(bur_fd-qnorm(0.975)*std0, lty = 6, col = "blue")
legend("topright", legend = c("Mean", "Mean + 2*SD", "Mean - 2*SD"), col = c("black", "red","blue"), lty = c(1, 6, 6), bty = "n")
mtext("Penalized splines for deseasoned data", side = 3, adj = 0)

plot(bur_fd1, ylim=c(-0.2,0.2), xlab="Year", ylab="Temperature", lty = 1, sub = "Figure 15")
title(main = "95% C.L. for Derivative Burlington", adj = 0, line = 1.1)
lines(bur_fd1+qnorm(0.975)*std1, lty = 6, col = "red")
lines(bur_fd1-qnorm(0.975)*std1, lty = 6, col = "blue")
legend("topright", legend = c("Mean", "Mean + 2*SD", "Mean - 2*SD"), col = c("black", "red","blue"), lty = c(1, 6, 6), bty = "n")
mtext("Penalized splines for deseasoned data", side = 3, adj = 0)
```




\newpage
# Permutation t-test

To see if there was a difference in the trends of the temperatures in various locations, a permutation test was employed to compare two sets of cities. Cities on the East Coast and cities on the West Coast were the groupings of interest. Cities on the East Coast are Burlington (VT), Miami (FL), New York City (NY), and Boston (MA). Cities on the West Coast are Los Angeles (CA), Portland (OR), Fairbanks (AK), and San Francisco (CA). Functional data plotted results showed in Figure 16 and Figure 17. It is obvious that both the temperature trend of two coast kept increasing from January 1971 to January 2023, although temperature trends of some western coastal cities had slight fluctuations.

```{r west-east, echo=FALSE, fig.height=7.5, fig.width=5}
par(mfrow=c(2,1))

east = smooth.basis(times, tempfd_deseasoned[,colnames(temp$east)], fdParobj = fdPar(new_bsbasis, Lfdobj = 4, lambda = lambdas[5]))$fd

west = smooth.basis(times, tempfd_deseasoned[,colnames(temp$west)], fdParobj = fdPar(new_bsbasis, Lfdobj = 4, lambda = lambdas[5]))$fd

plot(east, ylim = c(-2,2), xlab = "Year", ylab = "Temperature", sub = "Figure 16")
title(main = "East Coast Cities Penalized Splines", adj = 0, line = 1.1)
mtext("Order 6 B-spline basis on deseasoned data with K=10", side = 3, adj = 0)

plot(west, ylim = c(-2,2), xlab = "Year", ylab = "Temperature", sub = "Figure 17")
title(main = "West Coast Cities Penalized Splines", adj = 0, line = 1.1)
mtext("Order 6 B-spline basis on deseasoned data with K=10", side = 3, adj = 0)
```

\newpage
We rejected the null hypothesis (that two samples are the same) when observed statistics exceeded the point-wise critical value; otherwise, we did not succeed in doing so. Figure 18 demonstrated that two groups of cities occasionally had identical temperature trends, but occasionally did not. In this instance, our results were highly consistent with the author's findings, leading us to the conclusion that when combining the two coasts to form a comprehensive picture of the US, it may be appropriate to treat them separately rather than as a single unit, because they had overall similar increasing trend but couldn't be consider as the same.

```{r permutation, echo=FALSE}
# Permutation t-test
tperm.fd(east,west)$plot
title(main = "Permutation T-test for Two Groups", adj = 0, line = 1.1)
title(sub = "Figure 18")
mtext(" Compare splines of the west and east coast cities", side = 3, adj = 0)
```

# Summary
This study's main goal is to see if functional data analysis (FDA) techniques can be used to identify temperature variations in American cities during the past 60 years. The most difficult part of this simulation job was to remove seasonal effects and choose how to smoothing our data. Here we first fitted the fourier series to raw data with 6 basis functions and order 6 B-spline function with 109 basis function, then we took the difference to remove seasonal effect; finally we penalized the splines with fourth derivative and a smooth parameter. Then, we created 95% confidence limit for both mean and mean of derivative of de-seasoned functional data which also verified the increasing trend. In the end, we did the permutation t-test for two groups of cities, the findings indicate that temperatures have risen significantly in American cities over the past six decades which was not affected by the city's location on the east or west coast, and that the mean annual growth rate, which could represent overall annual temperature growth in the United States has been persistently above zero since the 1970s.
